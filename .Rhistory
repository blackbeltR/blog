install.packages("icon")
devtools::install_github("ropenscilabs/icon")
help("icon")
??icon
??fa_rocket
file.create(".nojekyll")
# importing posts
import_post("https://allanvc.github.io/post/web-scrap/2018-08-21-google_news_scraping/index.html",
date = as.Date("2017-12-24"))
radix::import_post("https://allanvc.github.io/post/web-scrap/2018-08-21-google_news_scraping/index.html",
date = as.Date("2017-12-24"))
radix::import_post("https://allanvc.github.io/post/web-scrap/2018-08-21-google_news_scraping/",
date = as.Date("2017-12-24"))
radix::import_post("https://github.com/allanvc/allanvc.github.io/tree/master/post/web-scrap/2018-08-21-google_news_scrapping/index.html",
date = as.Date("2017-12-24"))
radix::import_post(
"https://github.com/allanvc/allanvc.github.io/tree/master/post/web-scrap/2018-08-21-google_news_scrapping/index.html",
date = as.Date("2017-12-24"))
radix::import_post(
"https://github.com/allanvc/allanvc.github.io/tree/master/post/web-scrap/2018-08-21-google_news_scraping/index.html",
date = as.Date("2017-12-24"))
library(radix)
radix::import_post(
"https://github.com/allanvc/allanvc.github.io/tree/master/post/web-scrap/2018-08-21-google_news_scraping/index.html",
date = as.Date("2017-12-24"))
radix::import_post(
"https://therdojo.wordpress.com/2018/06/18/fiscaliza-fatura-fisfa-concurso-de-boas-praticas-da-cgu/",
date = as.Date("2017-12-24"))
library(radix)
create_post("Scraping Google News with 'rvest'")
create_post("Announcing the 'emstreR' package")
# como criar posts
library(radix)
create_post("Announcing the 'emstreR' package")
knitr::opts_chunk$set(echo = FALSE)
library(ggmap)
library(dplyr)
## define ports:
df.cities_brazil <- data.frame(location = c("Passo Fundo, Brazil",
"Buenos Aires, Argentina",
"Campinas, Brazil",
"Sao Paulo, Brazil",
"Ushuaia, Argentina",
"Belo Horizonte, Brazil",
"Brasilia, Brazil",
"Rio de Janeiro, Brazil",
"Campo Grande, Brazil",
"Recife, Brazil"
),
stringsAsFactors = FALSE)
df.cities_brazil
## get latitude and longitude:
geo.port_locations <- geocode(df.port_locations$location, source = "dsk")
## get latitude and longitude:
geo.port_locations <- geocode(df.cites_brazil$location, source = "dsk")
## define ports:
df.cities_brazil <- data.frame(location = c("Passo Fundo, Brazil",
"Buenos Aires, Argentina",
"Campinas, Brazil",
"Sao Paulo, Brazil",
"Ushuaia, Argentina",
"Belo Horizonte, Brazil",
"Brasilia, Brazil",
"Rio de Janeiro, Brazil",
"Campo Grande, Brazil",
"Recife, Brazil"
),
stringsAsFactors = FALSE)
## get latitude and longitude:
geo_locations <- geocode(df.cites_brazil$location, source = "dsk")
## get latitude and longitude:
geo_locations <- geocode(df.cities_brazil$location, source = "dsk")
View(df.cities_brazil)
View(geo_locations)
## define ports:
cities_location <- data.frame(location = c("Passo Fundo, Brazil",
"Buenos Aires, Argentina",
"Campinas, Brazil",
"Sao Paulo, Brazil",
"Ushuaia, Argentina",
"Belo Horizonte, Brazil",
"Brasilia, Brazil",
"Rio de Janeiro, Brazil",
"Campo Grande, Brazil",
"Recife, Brazil"
),
stringsAsFactors = FALSE)
## get latitude and longitude:
geo_location <- geocode(cities_location$location, source = "dsk")
## combine data:
df_location <- cbind(cities_location, geo_location)
View(geo_locations)
map_grid <- c(left = -8, bottom = 32, right = 20, top = 47)
View(df_location)
get_stamenmap(map_grid, zoom = 5) %>% ggmap()+
geom_point(data = df_location,
aes(x = lon, y = lat), size=3)
map_grid <- c(left = -20, bottom = 15, right = 5, top = 40)
get_stamenmap(map_grid, zoom = 5) %>% ggmap()+
geom_point(data = df_location,
aes(x = lon, y = lat), size=3)
map_grid <- c(left = -80, bottom = -10, right = -20, top = 20)
get_stamenmap(map_grid, zoom = 5) %>% ggmap()+
geom_point(data = df_location,
aes(x = lon, y = lat), size=3)
map_grid <- c(left = -80, bottom = -50, right = -30, top = 10)
get_stamenmap(map_grid, zoom = 5) %>% ggmap()+
geom_point(data = df_location,
aes(x = lon, y = lat), size=3)
map_grid <- c(left = -60, bottom = -40, right = -30, top = 5)
get_stamenmap(map_grid, zoom = 5) %>% ggmap()+
geom_point(data = df_location,
aes(x = lon, y = lat), size=3)
## cities:
cities_location <- data.frame(location = c("Passo Fundo, Brazil",
"Buenos Aires, Argentina",
"Campinas, Brazil",
"Ribeirao Preto, Brazil",
"Ushuaia, Argentina",
"Belo Horizonte, Brazil",
"Brasilia, Brazil",
"Rio de Janeiro, Brazil",
"Campo Grande, Brazil",
"Recife, Brazil"
),
stringsAsFactors = FALSE)
## get latitude and longitude:
geo_location <- geocode(cities_location$location, source = "dsk")
## combine data:
df_location <- cbind(cities_location, geo_location)
map_grid <- c(left = -65, bottom = -40, right = -30, top = 5)
get_stamenmap(map_grid, zoom = 5) %>% ggmap()+
geom_point(data = df_location,
aes(x = lon, y = lat), size=3)
View(df_location)
get_stamenmap(map_grid, zoom = 5) %>% ggmap()+
geom_point(data = df_location,
aes(x = lon, y = lat), size=2)
# coordinates for South America
map_grid <- c(left = -65, bottom = -60, right = -30, top = 5)
get_stamenmap(map_grid, zoom = 5) %>% ggmap()+
geom_point(data = df_location,
aes(x = lon, y = lat), size=2)
# coordinates for South America
map_grid <- c(left = -75, bottom = -60, right = -30, top = 5)
get_stamenmap(map_grid, zoom = 5) %>% ggmap()+
geom_point(data = df_location,
aes(x = lon, y = lat), size=2)
## cities:
cities_location <- data.frame(location = c("Passo Fundo, Brazil",
"Buenos Aires, Argentina",
"Campinas, Brazil",
"Ribeirao Preto, Brazil",
"Mendoza, Argentina",
"Belo Horizonte, Brazil",
"Brasilia, Brazil",
"Rio de Janeiro, Brazil",
"Campo Grande, Brazil",
"Recife, Brazil"
),
stringsAsFactors = FALSE)
## get latitude and longitude:
geo_location <- geocode(cities_location$location, source = "dsk")
## combine data:
df_location <- cbind(cities_location, geo_location)
# coordinates for South America
map_grid <- c(left = -75, bottom = -60, right = -30, top = 5)
get_stamenmap(map_grid, zoom = 5) %>% ggmap()+
geom_point(data = df_location,
aes(x = lon, y = lat), size=2)
knitr::opts_chunk$set(echo = FALSE)
library(ggmap)
## cities:
cities_location <- data.frame(location = c("Passo Fundo, Brazil",
"Buenos Aires, Argentina",
"Campinas, Brazil",
"Ribeirao Preto, Brazil",
"Mendoza, Argentina",
"Belo Horizonte, Brazil",
"Brasilia, Brazil",
"Rio de Janeiro, Brazil",
"Campo Grande, Brazil",
"Recife, Brazil"
),
stringsAsFactors = FALSE)
## get latitude and longitude
geo_location <- geocode(cities_location$location, source = "dsk")
## combine data:
df_location <- cbind(cities_location, geo_location)
## MST:
library(emstreeR)
out <- ComputeMST(df.location[,2:3], verbose = FALSE)
out <- ComputeMST(df_location[,2:3], verbose = FALSE)
out
get_stamenmap(map_grid, zoom = 5) %>% ggmap()+
stat_MST(data = out,
aes(x = lon, y = lat, from=from, to=to),
colour="red", linetype = 2)+
geom_point(data = out, aes(x = lon, y = lat), size=3)
knitr::opts_chunk$set(echo = FALSE)
library(ggmap)
## cities:
cities_location <- data.frame(location = c("Passo Fundo, Brazil",
"Buenos Aires, Argentina",
"Assuncion, Paraguay",
"Campinas, Brazil",
"Ribeirao Preto, Brazil",
"Mendoza, Argentina",
"Corrientes, Argentina",
"Porto Velho, Brazil",
"Manaus, Brazil",
"Santa Cruz de La Sierra, Bolivia",
"Belo Horizonte, Brazil",
"Brasília, Brazil",
"Rio de Janeiro, Brazil",
"Campo Grande, Brazil",
"Recife, Brazil"
),
stringsAsFactors = FALSE)
install.packages(c("blogdown", "radix"))
install.packages("icon")
install.packages("devtools")
devtools::install_version("icon")
Sys.Date()
rmarkdown::render_site()
devtools::install_git("https://github.com/ropenscilabs/icon")
install.packages(c("colorRamps", "ggthemes"))
install.packages(c("ggmap", "plotly"))
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
require(rvest) # para raspargem de dados
require(dplyr) # para uso de pipes
require(doSNOW) # processamento em paralelo
require(rvest) # para raspargem de dados
require(dplyr) # para uso de pipes
require(doSNOW) # processamento em paralelo
# TERMO DE BUSCA --------------------------------------------------------------
termo = "lenovo thinkpad" # adicione o termo de busca
# funcao para criar padrao das urls -------------------------------------------------------
get_url = function(termo) {
# prefixos da url
p1 = "https://lista.mercadolivre.com.br/%5B"
p2 = termo %>% strsplit(., " ") %>% unlist() %>% paste0(., collapse = "-")
p3 = "%5D_Desde_"
p4 = paste0(0, 1)
p5 = "_DisplayType_LF"
# juntando tudo
y = paste0(p1, p2, p3, p4)
qtd = read_html(y) %>%
html_nodes("aside div") %>%
html_text() %>%
{.[grepl("(resultados)$", .)]} %>%
gsub("\\D", "", .) %>%
as.numeric() %>%
{./50} %>%
ceiling() %>%
{1:.}
p4 = paste0((qtd - 1) * 5, 1)
y = paste0(p1, p2, p3, p4, p5)
return(y)
}
# chamando url padrao para o termo de busca
URLs = get_url(termo)
URLs
kable(URLs)
kable(URLs %>% head())
URLs %>%
head() %>%
as_tibble()
URLs %>%
head() %>%
as.data.frame()
URLs %>%
head() %>%
as.data.frame() %>%
rename(URLs = `.`)
URLs %>%
head() %>%
as.data.frame() %>%
rename(URLs = ".")
URLs %>%
head() %>%
as.data.frame() %>%
rename(URLs = ".") %>%
kable()
URLs %>%
head() %>%
as.data.frame() %>%
rename(URLs = ".") %>%
kable(align = "c", caption = "Padrão de URLs do ML")
# funcao para pegar os links
get_links = function(URL) {
y = read_html(URL) %>%
html_nodes("div h2 a") %>%
html_attr("href") %>%
{.[grepl("mercadolivre.com.br", .)]} %>%
unique() %>%
unlist()
return(y)
}
# pegar links dos produtos
url.produtos = NULL
for (i in 1:length(URLs)) {
res = try({
url.produtos = append(url.produtos, get_links(URL = URLs[i]))
})
if(inherits(res, "try-error")) {
break
}
}
url.produtos
url.produtos %>% head() %>%
as.data.frame() %>%
rename(URLs = ".") %>%
kable(align = "c", caption = "Padrão de URLs das páginas do ML")
length(URLs)
paste0(length(URLs), " páginas de produtos")
paste0(length(url.produtos), " produtos localizados")
paste0(length(url.produtos), " produtos localizados seguindo o termo de busca: ", termo)
get_atr_anuncio = function(url.prod) {
y = read_html(url.prod)
# titulo
titulo = y %>%
html_nodes("div header") %>%
html_text() %>%
gsub("(\n|\t)", "", .) %>%
stringi::stri_trans_general(., "Latin-ASCII")
if(length(titulo) == 0) {
titulo = NA
}
# descricao
descricao = y %>%
html_nodes("section div p") %>%
html_text() %>%
{.[2]} %>%
stringi::stri_trans_general(., "Latin-ASCII")
if(length(descricao) == 0) {
descricao = NA
}
# valor
valor = y %>%
html_nodes("fieldset span") %>%
html_attr("content") %>%
{.[!is.na(.)]} %>%
as.numeric()
if(length(valor) == 0) {
valor = NA
}
# qtd vendidos
qtd_vendidos = y %>%
html_nodes("dl div") %>%
gsub('(<div class="item-conditions">|</div>)', "", .) %>%
gsub("(\n|\t)", " ", .) %>%
gsub("\\D", "", .) %>%
as.numeric()
if(length(qtd_vendidos) == 0) {
qtd_vendidos = 0
}
y = cbind.data.frame(titulo, descricao, valor, qtd_vendidos)
return(y)
}
cores = parallel::detectCores() # lendo quantos cores sua maquina tem
cl = makeSOCKcluster(cores) # criando os clusters de trabalho em paralelo
registerDoSNOW(cl) # registrando os clusters no sistema
pb = txtProgressBar(min = 1, max = length(url.produtos), style = 3) # barrinha de progresso
progress = function(n) setTxtProgressBar(pb, n) # colocando a barra de progresso como uma funcao de n
opts = list(progress = progress) # adicionando lista de opcoes com a barra de progresso
base = foreach(i = 1:length(url.produtos), # criando a base de dados em paralelo
.options.snow = opts,
.combine = 'rbind',
.packages = c("stringi", "dplyr", "rvest")) %dopar% {
get_atr_anuncio(url.prod = url.produtos[i])
}
close(pb) # fechando a barra de progresso
stopCluster(cl) # parando os clusters de trabalho
base = base[complete.cases(base), ] # pegando apenas as linhas com informacoes completas
base %>%
head() %>%
kable(align = "c", caption = "Primeiras obervações da base de dados com o termo de busca: ", termo)
base %>%
head() %>%
kable(align = "c", caption = paste0("Primeiras obervações da base de dados com o termo de busca: ", termo))
# verifidando medidas resumo para o termo de busca
summary(base$valor)
# imprimindo as primeiras entradas
url.produtos %>%
head()
paste0(length(url.produtos), " produtos localizados seguindo o termo de busca: ", termo)
