---
title: "Scraping Google News with 'rvest'"
description: |
  This is an example of how to scrape Google News website with the awesome rvest package.
  Originally posted on https://allanvc.github.io.
preview: ./img/google_news_screenshot.png
creative_commons: CC BY
author:
  - name: Allan V. C. Quadros
    url: https://allanvc.github.io
date: 12-24-2018
output:
  radix::radix_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

This is an example of how to scrape Google News with the awesome `rvest` package. 

This work is a solution for a question that appeared in our WhatsApp group, [**blackbeltR**](https://www.blackbeltR.com.br){target="_blank"}. A user came up with this problem and I decided to help him. It was a cool challenge, so why not?

A great deal of the basic ideas comes from his own code. I just kept it and added few things in order to get the code working. 

First off, you should take a look at the Google News website [HERE](https://news.google.com/){target="_blank"}, which I reproduce below:

<!-- criamos uma pasta /img em /_post/nome_do_post/ -> essa eh o diretorio raiz do post  -->
  
![](./img/google_news_screenshot.png)

---

You may notice, on the right side of the page, that we are using Google Chrome **dev-tools**. We use this to identify the *html nodes* we need. You can access this tool by hitting the **F12** key. The html nodes are passed as arguments to the `rvest` functions.

Basically, the idea is to extract the communication vehicle (vehicle), the time elapsed since the news was published (time), and the main headline (headline).

The code and coments are presented below: 

```{r, message=FALSE}
# loading the packages:
library(dplyr) # for pipes and the data_frame function
library(rvest) # webscraping
library(stringr) # to deal with strings and to clean up our data
```


```{r}
# extracting the whole website
google <- read_html("https://news.google.com/")
```


```{r}
# extracting the com vehicles
# we pass the nodes in html_nodes and extract the text from the last one 
# we use stringr to delete strings that are not important
vehicle_all <- google %>% 
  html_nodes("div div div main c-wiz div div div article div div div") %>% 
  html_text() %>%
  str_subset("[^more_vert]") %>%
  str_subset("[^share]") %>%
  str_subset("[^bookmark_border]")

vehicle_all[1:10] # take a look at the first ten

```



```{r}
# extracting the time elapsed
time_all <- google %>% html_nodes("div article div div time") %>% html_text()

time_all[1:10] # take a look at the first ten
```


```{r}
# extracting the headlines
# and using stringr for cleaning
headline_all <- google %>% html_nodes("article") %>% html_text("span") %>%
  str_split("(?<=[a-z0-9'!?\\.])(?=[A-Z])")

headline_all <- sapply(headline_all, function(x) x[1]) # extract only the first elements


headline_all[1:10] # take a look at the first ten
```

In this last case we used a regular expression (REGEX) to clean up the data. We did this by separating the actual headline phrases from the complementary ones. In some cases, we have a phrase ending with uppercase letters such as "NSA" (a Brazilian state) collapsed with other phrase initiating with another uppercase letter such as the word "A" ("...with NSAA agent said...") for example. We have to think of a better way to split these cases, but the current result is quite satisfactory for now. 

The expression `?<=` is called "lookbehind", while `?=` is called "lookahead". Those "lookaround" expressions allow us to look for patterns followed or preceded by something. In our case, the idea is basically to separate a string at the point in which lowercase letters, numbers, exclamation points or question marks (`[a-z0-9!?]`) are collapsed with uppercase letters (`[A-Z]`), e.g. where lowercase letters, numbers and others are preceded (`?<=`) by uppercase letters.

Before we finish, we have to clean up our data. It is common to collect garbage in the process such as data related to "fact checking", which is a section on the right side of the page. As a result, it is possible that the three vectors we have created may have different sizes. Therefore, we use the smallest of them as the base and just delete the entries above this number on the other two vectors.

```{r}
# finding the smallest vector
min <- min(sapply(list(vehicle_all, time_all, headline_all), length))

# cutting
vehicle_all <- vehicle_all[1:min]
time_all <- time_all[1:min]
headline_all <- headline_all[1:min]

```


And we have our final data frame:

```{r}

df_news <- data_frame(vehicle_all, time_all, headline_all)

df_news
```
